/**
 * This file was auto-generated by Fern from our API Definition.
 */

import * as environments from "../../../../../../../../environments";
import * as core from "../../../../../../../../core";
import * as ElevenLabs from "../../../../../../../index";
import { mergeHeaders, mergeOnlyDefinedHeaders } from "../../../../../../../../core/headers.js";
import * as serializers from "../../../../../../../../serialization/index";
import urlJoin from "url-join";
import * as errors from "../../../../../../../../errors/index";

export declare namespace LlmUsage {
    export interface Options {
        environment?: core.Supplier<environments.ElevenLabsEnvironment | environments.ElevenLabsEnvironmentUrls>;
        /** Specify a custom URL to connect the client to. */
        baseUrl?: core.Supplier<string>;
        /** Override the xi-api-key header */
        apiKey?: core.Supplier<string | undefined>;
        /** Additional headers to include in requests. */
        headers?: Record<string, string | core.Supplier<string | undefined> | undefined>;
    }

    export interface RequestOptions {
        /** The maximum time to wait for a response in seconds. */
        timeoutInSeconds?: number;
        /** The number of times to retry the request. Defaults to 2. */
        maxRetries?: number;
        /** A hook to abort the request. */
        abortSignal?: AbortSignal;
        /** Override the xi-api-key header */
        apiKey?: string | undefined;
        /** Additional headers to include in the request. */
        headers?: Record<string, string | core.Supplier<string | undefined> | undefined>;
    }
}

export class LlmUsage {
    protected readonly _options: LlmUsage.Options;

    constructor(_options: LlmUsage.Options = {}) {
        this._options = _options;
    }

    /**
     * Calculates expected number of LLM tokens needed for the specified agent.
     *
     * @param {string} agentId
     * @param {ElevenLabs.conversationalAi.agents.LlmUsageCalculatorRequestModel} request
     * @param {LlmUsage.RequestOptions} requestOptions - Request-specific configuration.
     *
     * @throws {@link ElevenLabs.UnprocessableEntityError}
     *
     * @example
     *     await client.conversationalAi.agents.llmUsage.calculate("agent_id")
     */
    public calculate(
        agentId: string,
        request: ElevenLabs.conversationalAi.agents.LlmUsageCalculatorRequestModel = {},
        requestOptions?: LlmUsage.RequestOptions,
    ): core.HttpResponsePromise<ElevenLabs.LlmUsageCalculatorResponseModel> {
        return core.HttpResponsePromise.fromPromise(this.__calculate(agentId, request, requestOptions));
    }

    private async __calculate(
        agentId: string,
        request: ElevenLabs.conversationalAi.agents.LlmUsageCalculatorRequestModel = {},
        requestOptions?: LlmUsage.RequestOptions,
    ): Promise<core.WithRawResponse<ElevenLabs.LlmUsageCalculatorResponseModel>> {
        const _response = await core.fetcher({
            url: urlJoin(
                (await core.Supplier.get(this._options.baseUrl)) ??
                    (
                        (await core.Supplier.get(this._options.environment)) ??
                        environments.ElevenLabsEnvironment.Production
                    ).base,
                `v1/convai/agent/${encodeURIComponent(agentId)}/llm-usage/calculate`,
            ),
            method: "POST",
            headers: mergeHeaders(
                this._options?.headers,
                mergeOnlyDefinedHeaders({ "xi-api-key": requestOptions?.apiKey }),
                requestOptions?.headers,
            ),
            contentType: "application/json",
            requestType: "json",
            body: serializers.conversationalAi.agents.LlmUsageCalculatorRequestModel.jsonOrThrow(request, {
                unrecognizedObjectKeys: "strip",
            }),
            timeoutMs: requestOptions?.timeoutInSeconds != null ? requestOptions.timeoutInSeconds * 1000 : 240000,
            maxRetries: requestOptions?.maxRetries,
            abortSignal: requestOptions?.abortSignal,
        });
        if (_response.ok) {
            return {
                data: serializers.LlmUsageCalculatorResponseModel.parseOrThrow(_response.body, {
                    unrecognizedObjectKeys: "passthrough",
                    allowUnrecognizedUnionMembers: true,
                    allowUnrecognizedEnumValues: true,
                    breadcrumbsPrefix: ["response"],
                }),
                rawResponse: _response.rawResponse,
            };
        }

        if (_response.error.reason === "status-code") {
            switch (_response.error.statusCode) {
                case 422:
                    throw new ElevenLabs.UnprocessableEntityError(
                        serializers.HttpValidationError.parseOrThrow(_response.error.body, {
                            unrecognizedObjectKeys: "passthrough",
                            allowUnrecognizedUnionMembers: true,
                            allowUnrecognizedEnumValues: true,
                            breadcrumbsPrefix: ["response"],
                        }),
                        _response.rawResponse,
                    );
                default:
                    throw new errors.ElevenLabsError({
                        statusCode: _response.error.statusCode,
                        body: _response.error.body,
                        rawResponse: _response.rawResponse,
                    });
            }
        }

        switch (_response.error.reason) {
            case "non-json":
                throw new errors.ElevenLabsError({
                    statusCode: _response.error.statusCode,
                    body: _response.error.rawBody,
                    rawResponse: _response.rawResponse,
                });
            case "timeout":
                throw new errors.ElevenLabsTimeoutError(
                    "Timeout exceeded when calling POST /v1/convai/agent/{agent_id}/llm-usage/calculate.",
                );
            case "unknown":
                throw new errors.ElevenLabsError({
                    message: _response.error.errorMessage,
                    rawResponse: _response.rawResponse,
                });
        }
    }
}
