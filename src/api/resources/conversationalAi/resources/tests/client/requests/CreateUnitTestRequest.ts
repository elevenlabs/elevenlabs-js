// This file was auto-generated by Fern from our API Definition.

import type * as ElevenLabs from "../../../../../../index";

/**
 * @example
 *     {
 *         chatHistory: [{
 *                 role: "user",
 *                 timeInCallSecs: 1
 *             }],
 *         successCondition: "success_condition",
 *         successExamples: [{
 *                 response: "response",
 *                 type: "success"
 *             }],
 *         failureExamples: [{
 *                 response: "response",
 *                 type: "failure"
 *             }],
 *         name: "name"
 *     }
 */
export interface CreateUnitTestRequest {
    chatHistory: ElevenLabs.ConversationHistoryTranscriptCommonModelInput[];
    /** A prompt that evaluates whether the agent's response is successful. Should return True or False. */
    successCondition: string;
    /** Non-empty list of example responses that should be considered successful */
    successExamples: ElevenLabs.AgentSuccessfulResponseExample[];
    /** Non-empty list of example responses that should be considered failures */
    failureExamples: ElevenLabs.AgentFailureResponseExample[];
    /** How to evaluate the agent's tool call (if any). If empty, the tool call is not evaluated. */
    toolCallParameters?: ElevenLabs.UnitTestToolCallEvaluationModelInput;
    /** If set to True this test will pass if any tool call returned by the LLM matches the criteria. Otherwise it will fail if more than one tool is returned by the agent. */
    checkAnyToolMatches?: boolean;
    /** Dynamic variables to replace in the agent config during testing */
    dynamicVariables?: Record<
        string,
        ElevenLabs.conversationalAi.CreateUnitTestRequestDynamicVariablesValue | undefined
    >;
    type?: ElevenLabs.UnitTestCommonModelType;
    /** Metadata of a conversation this test was created from (if applicable). */
    fromConversationMetadata?: ElevenLabs.TestFromConversationMetadataInput;
    name: string;
}
