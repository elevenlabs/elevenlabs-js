// This file was auto-generated by Fern from our API Definition.

import { ElevenLabsClient } from "../../../src/Client";
import { mockServerPool } from "../../mock-server/MockServerPool";

describe("LlmUsageClient", () => {
    test("calculate", async () => {
        const server = mockServerPool.createServer();
        const client = new ElevenLabsClient({ maxRetries: 0, apiKey: "test", environment: server.baseUrl });
        const rawRequestBody = { prompt_length: 1, number_of_pages: 1, rag_enabled: true };
        const rawResponseBody = { llm_prices: [{ llm: "gpt-4o-mini", price_per_minute: 1.1 }] };
        server
            .mockEndpoint()
            .post("/v1/convai/llm-usage/calculate")
            .jsonBody(rawRequestBody)
            .respondWith()
            .statusCode(200)
            .jsonBody(rawResponseBody)
            .build();

        const response = await client.conversationalAi.llmUsage.calculate({
            promptLength: 1,
            numberOfPages: 1,
            ragEnabled: true,
        });
        expect(response).toEqual({
            llmPrices: [
                {
                    llm: "gpt-4o-mini",
                    pricePerMinute: 1.1,
                },
            ],
        });
    });
});
