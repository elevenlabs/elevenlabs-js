/**
 * This file was auto-generated by Fern from our API Definition.
 */

import { mockServerPool } from "../../mock-server/MockServerPool.js";
import { ElevenLabsClient } from "../../../src/Client";

describe("LlmUsage", () => {
    test("calculate", async () => {
        const server = mockServerPool.createServer();
        const client = new ElevenLabsClient({
            apiKey: "test",
            environment: { base: server.baseUrl, wss: server.baseUrl },
        });
        const rawRequestBody = { prompt_length: 1, number_of_pages: 1, rag_enabled: true };
        const rawResponseBody = { llm_prices: [{ llm: "gpt-4o-mini", price_per_minute: 1.1 }] };
        server
            .mockEndpoint()
            .post("/v1/convai/llm-usage/calculate")
            .jsonBody(rawRequestBody)
            .respondWith()
            .statusCode(200)
            .jsonBody(rawResponseBody)
            .build();

        const response = await client.conversationalAi.llmUsage.calculate({
            promptLength: 1,
            numberOfPages: 1,
            ragEnabled: true,
        });
        expect(response).toEqual({
            llmPrices: [
                {
                    llm: "gpt-4o-mini",
                    pricePerMinute: 1.1,
                },
            ],
        });
    });
});
